'''
Using clickstream data, create DataFrame with columns 
'title', 'month', 'external_pv', 'internal_pv'
'''

# import argparse
import pandas as pd
import pathlib
# import pickle
# import scipy.sparse as ss
import time

st = time.time()

def get_external_and_internal_pv(zip_file, month_name):
    '''
    Inputs:
        zip_file: Path to gzipped file containing Wikipedia clickstream data 
        # title_to_idx_file: Path to pickle file generated by process_wikilinkgraph.py
    '''
    # Read in files
    print('File: ', zip_file)
    df = pd.read_csv(zip_file, compression='gzip', sep='\t',
                # nrows=1000, # Can uncomment when debugging
                names=['prev', 'curr', 'type', 'n'])
    num_rows = len(df)

    # print(df[df['curr']=="Dallas_Tornado"].head(30))
    # print(df['curr'].unique())
    print('Number of rows:', num_rows)

    # Filter out rows of type other (correspond to non-existent edges)
    df = df[~(df['type']=='other')]
    print('Number of rows with type != other:', len(df))
    print('Number of rows with type = external:', len(df[df['type']=='external']))
    print('Number of rows with type = link:', len(df[df['type']=='link']))

    # TODO: Think about whether it is necessary to filter out internal page views
    # that come from pages not in WikiLinkGraph
    # with open(title_to_idx_file, 'rb') as f:
    #     title_to_idx = pickle.load(f)
    # num_pages = len(title_to_idx)

    # # print(df.head())
    # # Convert underscores in page titles to spaces
    # print('Converting underscores in page titles to spaces...')
    # df['prev'] = df['prev'].apply(lambda x: str(x).replace('_', ' '))
    # df['curr'] = df['curr'].apply(lambda x: str(x).replace('_', ' '))
    # print(df.head())

    # # There may be a mismatch between the pages that exist in WikiLinkGraphs and the 
    # # clickstream files, so we exclude rows that are marked 'internal' but one or both
    # # of the pages are not in title_to_idx
    # num_rows_before = len(df)

    # print('**** Rows with curr not in WikiLinkGraphs:')
    # print(df['curr'][~(df['curr'].isin(title_to_idx))])

    # idx1 = (df['type'] == 'link') & ~(df['prev'].isin(title_to_idx))
    # idx2 = ~(df['curr'].isin(title_to_idx))
    # df = df[~(idx1 | idx2)]

    # # df = df[~((df['type'] == 'link') & (~(df['prev'].isin(title_to_idx)) | ~(df['curr'].isin(title_to_idx))))]
    # print(f'Excluded {num_rows_before - len(df)} rows that include pages not present in WikiLinkGraphs')

    external_pv = df[df['type']=='external'].groupby('curr', as_index=False).sum()
    external_pv.rename(columns={'n': 'external_pv'}, inplace=True)

    internal_pv = df[df['type']=='link'].groupby('curr', as_index=False).sum()
    internal_pv.rename(columns={'n': 'internal_pv'}, inplace=True)

    new_df = pd.merge(external_pv, internal_pv, how='outer')
    new_df = new_df.fillna(0) # Convert missing pv counts from NaN to 0
    new_df['month'] = month_name

    return new_df

def get_all_external_and_internal_pv(zip_file_list, month_names, save_to=None):

    frames = [get_external_and_internal_pv(zip_file, month_name) 
                for zip_file, month_name in zip(zip_file_list, month_names)]
    combined_df = pd.concat(frames)

    combined_df.rename(columns={'curr': 'title'}, inplace=True)
    summed = combined_df.groupby('title').sum()

    if save_to is not None:
        summed.to_csv(save_to)
        print(f'Saved aggregated external & internal page views to {save_to}')

    return summed

if __name__ == '__main__':

    st = time.time()

    # ---------------------------------------
    months = ["2018-01", "2018-02", "2018-03", "2018-04", "2018-05", "2018-06", 
        "2018-07", "2018-08", "2018-09", "2018-10", "2018-11", "2018-12"]

    zip_file_list = [f'data/clickstream/clickstream-enwiki-{month}.tsv.gz' for month in months]
    save_to = 'page_views_analysis/data/external_and_internal_page_views.csv'
    pathlib.Path("page_views_analysis/data").mkdir(parents=True, exist_ok=True)

    get_all_external_and_internal_pv(zip_file_list, months, save_to=save_to)

    # ---------------------------------------

    print(f'Time taken: {(time.time() - st) / 60:.2f} min')